apiVersion: sagemaker.amazonaws.com/v1
kind: HyperPodPyTorchJob
metadata:
  name: llama3-1-8b-fsdp-hpto
spec:
  nprocPerNode: "1"
  runPolicy:
    jobMaxRetryCount: 0
    restartPolicy:
      numRestartBeforeFullJobRestart: 0
      evalPeriodSeconds: 21600
      maxFullJobRestarts: 0
    cleanPodPolicy: All
    logMonitoringConfiguration:
      - name: JobStart
        logPattern: '.*Loss:.*'
        expectedStartCutOffInSeconds: 240
      - name: JobHangingDetection
        logPattern: '.*Loss:.*'
        expectedRecurringFrequencyInSeconds: 600
  replicaSpecs:
    - name: pods
      replicas: 8
      template:
        metadata:
          labels:
            job-name: llama3-1-8b-fsdp-hpto
            replica-type: pods
        spec:
          volumes:
            - name: shmem
              hostPath:
                path: /dev/shm
            - name: local
              hostPath:
                path: /mnt/k8s-disks/0
            - name: fsx-pv
              persistentVolumeClaim:
                claimName: fsx-claim
          affinity:
            podAntiAffinity:
              requiredDuringSchedulingIgnoredDuringExecution:
                - labelSelector:
                    matchLabels:
                      job-name: llama3-1-8b-fsdp-hpto
                  topologyKey: kubernetes.io/hostname
            nodeAffinity:
              requiredDuringSchedulingIgnoredDuringExecution:
                nodeSelectorTerms:
                  - matchExpressions:
                      - key: sagemaker.amazonaws.com/node-health-status
                        operator: In
                        values:
                          - Schedulable
                      - key: sagemaker.amazonaws.com/compute-type
                        operator: In
                        values:
                          - 'ml.g5.8xlarge'
          topologySpreadConstraints:
            - maxSkew: 1
              topologyKey: kubernetes.io/hostname
              whenUnsatisfiable: DoNotSchedule
              labelSelector:
                matchLabels:
                  job-name: llama3-1-8b-fsdp-hpto

          serviceAccountName: riv2025-aim365-demo-service-account

          containers:
            - name: pytorch
              image: 842413447717.dkr.ecr.us-east-2.amazonaws.com/riv2025-aim365-demo:pytorch2.5.1
              imagePullPolicy: Always
              resources:
                requests:
                  nvidia.com/gpu: 1
                  vpc.amazonaws.com/efa: 1
                limits:
                  nvidia.com/gpu: 1
                  vpc.amazonaws.com/efa: 1
              env:
                - name: LOGLEVEL
                  value: INFO
                # - name: FI_PROVIDER
                #   value: efa
                # - name: FI_EFA_USE_DEVICE_RDMA
                #   value: '1'
                # - name: FI_EFA_FORK_SAFE
                #   value: '1'
                # - name: FI_EFA_ENABLE_SHM_TRANSFER
                #   value: '1'
                - name: TORCH_DISTRIBUTED_DEBUG
                  value: DETAIL
                - name: TORCH_NCCL_ENABLE_MONITORING
                  value: '1'
                - name: TORCH_NCCL_TRACE_BUFFER_SIZE
                  value: '20000'
                - name: TORCH_NCCL_DUMP_ON_TIMEOUT
                  value: '1'
                - name: TORCH_NCCL_DEBUG_INFO_TEMP_FILE
                  value: /local/nccl_trace_rank_
                - name: PYTORCH_CUDA_ALLOC_CONF
                  value: 'expandable_segments:True'
                - name: NCCL_DEBUG
                  value: INFO
                - name: NCCL_SOCKET_IFNAME
                  value: ^lo
                - name: TORCH_NCCL_ASYNC_ERROR_HANDLING
                  value: '1'
                # HF_TOKEN not needed for public datasets
                # - name: HF_TOKEN
                #   value: 'your_token_here'
              command:
                - hyperpodrun
                - '--tee=3'
                - '--log_dir=/tmp/hyperpod'
                - '--nproc_per_node=1'
                - '--nnodes=8'
                - /fsdp/train.py
                - '--max_context_width=4096'
                - '--num_key_value_heads=32'
                - '--intermediate_size=11008'
                - '--hidden_width=4096'
                - '--num_layers=16'
                - '--num_heads=32'
                - '--model_type=llama_v2'
                - '--tokenizer=hf-internal-testing/llama-tokenizer'
                - '--checkpoint_freq=50'
                - '--validation_freq=25'
                - '--max_steps=1000'
                - '--checkpoint_dir=/fsx/checkpoints'
                # Use local dataset instead of streaming
                - '--local_dataset_path=/fsx/c4_subset'
                - '--resume_from_checkpoint=/fsx/checkpoints'
                - '--train_batch_size=1'
                - '--val_batch_size=1'
                - '--sharding_strategy=full'
                - '--offload_activations=1'
              volumeMounts:
                - name: shmem
                  mountPath: /dev/shm
                - name: local
                  mountPath: /local
                - name: fsx-pv
                  mountPath: /fsx
