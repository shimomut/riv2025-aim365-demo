# FSDP Training on SageMaker HyperPod

This project provides a comprehensive FSDP (Fully Sharded Data Parallel) training application designed for SageMaker HyperPod environments. It supports distributed training of large language models including Llama 2, Llama 3.x, Mistral, and Mathstral models using PyTorch FSDP.

## Quick Start

### 1. Validate Setup
```bash
make validate-setup
```

### 2. Run Training
```bash
make run
```

### 3. Monitor Progress
```bash
make logs-follow
```

### 4. Stop Training
```bash
make stop
```

## Documentation

ğŸ“š **Complete documentation is available in the [`doc/`](doc/) directory:**

- **[Testing Guide](doc/TESTING_GUIDE.md)** - Comprehensive testing procedures
- **[Quick Reference](doc/TESTING_QUICK_REFERENCE.md)** - Essential commands
- **[Verification Results](doc/VERIFICATION_RESULTS.md)** - Proof of functionality
- **[Documentation Index](doc/README.md)** - Full documentation overview

## Key Features

- âœ… **Verified Working** - Tested on actual HyperPod clusters
- ğŸš€ **One-Command Deployment** - `make run` to start training
- ğŸ“Š **Comprehensive Testing** - Automated test suite with monitoring
- ğŸ”§ **Easy Debugging** - Built-in troubleshooting commands
- ğŸ“ˆ **Performance Monitoring** - Real-time metrics and logging
- ğŸ›¡ï¸ **Fault Tolerance** - HyperPod auto-recovery integration

## Supported Models

- Llama 2 (7B, 13B, 70B)
- Llama 3.1 (8B, 70B) 
- Llama 3.2 (1B, 3B)
- Mistral 8x7B
- Mistral Mathstral 7B

## Project Structure

```
â”œâ”€â”€ FSDP/                    # Core training code
â”‚   â”œâ”€â”€ src/                 # Training scripts and utilities
â”‚   â””â”€â”€ kubernetes/          # HyperPod job configurations
â”œâ”€â”€ doc/                     # Documentation
â”œâ”€â”€ tools/                   # Utilities and scripts
â”‚   â”œâ”€â”€ test_hyperpod_cluster.py # Testing framework
â”‚   â””â”€â”€ validate_test_setup.py   # Setup validation
â””â”€â”€ Makefile                 # Testing and deployment commands
```

## Requirements

- SageMaker HyperPod cluster with GPU nodes
- kubectl configured for cluster access
- AWS CLI configured
- Container images built and pushed to ECR

## Getting Help

Run `make help` to see all available commands, or check the [documentation](doc/) for detailed guides.